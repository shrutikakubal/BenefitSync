{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4fd99d98-4820-4627-be43-376d212d0695",
   "metadata": {},
   "source": [
    "1.3.2. Phase 2: Policy Document Indexing for RAG\n",
    "\n",
    "Index benefits policy PDFs for Retrieval-Augmented Generation (RAG) to enable\n",
    "efficient policy retrieval.\n",
    "\n",
    "Text Extraction: Use PyPDF2 to extract text from policy PDFs, removing noise\n",
    "(e.g., page numbers).\n",
    "\n",
    "Text Preprocessing: Segment text into chunks (e.g., 200 words per chunk).\n",
    "\n",
    "Vector Store Creation: Build a Chroma vector store using embeddings from \n",
    "sentence-transformers or OpenAI.\n",
    "\n",
    "Retrieval Testing: Test retrieval with 5 HR policy queries (e.g., “What is the\n",
    "eligibility for Tuition Reimbursement?”) and evaluate chunk relevance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a5de3da5-44ef-4baa-ac25-643fe636b661",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting PyPDF2\n",
      "  Downloading pypdf2-3.0.1-py3-none-any.whl.metadata (6.8 kB)\n",
      "Downloading pypdf2-3.0.1-py3-none-any.whl (232 kB)\n",
      "Installing collected packages: PyPDF2\n",
      "Successfully installed PyPDF2-3.0.1\n"
     ]
    }
   ],
   "source": [
    "!pip install PyPDF2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "id": "3d644c3d-72a3-4a3d-8d11-fd161c38abc4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found PDF: ./assets/benefits/gym-policy.pdf\n",
      "Total pages: 7\n",
      "Found PDF: ./assets/benefits/life-insurance-policy.pdf\n",
      "Total pages: 8\n",
      "Found PDF: ./assets/benefits/childcare-policy.pdf\n",
      "Total pages: 8\n",
      "Found PDF: ./assets/benefits/401k-retirement-policy.pdf\n",
      "Total pages: 9\n",
      "Found PDF: ./assets/benefits/work-from-home-policy.pdf\n",
      "Total pages: 7\n",
      "Found PDF: ./assets/benefits/vacation-policy.pdf\n",
      "Total pages: 7\n",
      "Found PDF: ./assets/benefits/health-insurance-policy.pdf\n",
      "Total pages: 7\n",
      "Found PDF: ./assets/benefits/tuition-reimbursement-policy.pdf\n",
      "Total pages: 8\n"
     ]
    }
   ],
   "source": [
    "from PyPDF2 import PdfReader\n",
    "from langchain_text_splitters import RecursiveCharacterTextSplitter,CharacterTextSplitter\n",
    "import os\n",
    "import pandas as pd\n",
    "\n",
    "directory_path = './assets/benefits/'  # Replace with the actual directory path\n",
    "\n",
    "pdf_rows = []\n",
    "text_splitter = CharacterTextSplitter(separator='\\n\\n',chunk_size=200,chunk_overlap=0)\n",
    "\n",
    "for filename in os.listdir(directory_path):\n",
    "    if filename.endswith('.pdf'):\n",
    "        full_path = os.path.join(directory_path, filename)\n",
    "        print(f\"Found PDF: {full_path}\")\n",
    "        reader = PdfReader(full_path)\n",
    "        total_pages = len(reader.pages)\n",
    "        print(f\"Total pages: {total_pages}\")\n",
    "        for i in range(total_pages):\n",
    "            chunk_num = 0\n",
    "            page_text = reader.pages[i].extract_text()\n",
    "            page_text = page_text.replace(\"Introduction\", '')\n",
    "            chunks = text_splitter.split_text(page_text)\n",
    "            for c in chunks:\n",
    "                pdf_rows.append({'policy': filename, 'page': i, 'chunk': chunk_num,'text': c})\n",
    "                chunk_num+=1\n",
    "policy_pdfs = pd.DataFrame(pdf_rows)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10901cb6-5611-4c57-bbcf-43a0042dd90f",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install -qU \"langchain-chroma\"\n",
    "!pip install -qU langchain-openai"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "id": "fae210ac-c6e0-42df-905e-d0573f5827e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import getpass\n",
    "import os\n",
    "from langchain_openai import OpenAIEmbeddings\n",
    "\n",
    "if not os.environ.get(\"OPENAI_API_KEY\"):\n",
    "  os.environ[\"OPENAI_API_KEY\"] = getpass.getpass(\"Enter API key for OpenAI: \")\n",
    "embeddings = OpenAIEmbeddings(model=\"text-embedding-3-large\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51990fcc-0398-4584-a8d2-626524888056",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_chroma import Chroma\n",
    "from langchain_core.documents import Document\n",
    "from uuid import uuid4\n",
    "\n",
    "vector_store = Chroma(collection_name=\"policy_embeddings\",embedding_function=embeddings,\n",
    "    persist_directory=\"./assets/chroma\")\n",
    "\n",
    "docs = []\n",
    "idx = 0\n",
    "#vector_store.delete(ids=uuids[:-1])\n",
    "for row in policy_pdfs.itertuples():\n",
    "    doc = Document(page_content=row.text, metadata={'policy': row.policy, 'page': row.page}, id=idx)\n",
    "    docs.append(doc)\n",
    "    idx+=1\n",
    "uuids = [str(uuid4()) for _ in range(len(docs))]\n",
    "vector_store.add_documents(documents=docs, ids=uuids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f915a092-c5e1-4281-95c3-afbab7288494",
   "metadata": {},
   "outputs": [],
   "source": [
    "results = vector_store.similarity_search_with_score(\n",
    "    \"can i join the gym?\", k=2\n",
    ")\n",
    "for res, score in results:\n",
    "    print(f\"* [Similarity ={score:3f}] {res.page_content} [{res.metadata}]\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "conda_python3",
   "language": "python",
   "name": "conda_python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
